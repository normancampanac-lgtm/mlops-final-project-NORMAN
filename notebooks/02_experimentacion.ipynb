{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c406f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Experimentaci√≥n y Modelado - Predicci√≥n de Precios de Casas en California\n",
    "# \n",
    "# **Proyecto:** mlops-final-project_1  \n",
    "# **Autor:** [NORMAN CAMPANA](https://www.linkedin.com/in/normancampana/)  \n",
    "# **Fecha:** $(datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "# \n",
    "# ## üéØ Objetivo\n",
    "# Experimentar con m√∫ltiples algoritmos de Machine Learning para encontrar el mejor modelo que prediga precios de viviendas en California.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìã Configuraci√≥n Inicial\n",
    "\n",
    "# %%\n",
    "# ============================================\n",
    "# CONFIGURACI√ìN DE IMPORTACIONES Y PATHS\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# A√±adir el directorio src al path para poder importar m√≥dulos propios\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Configurar rutas de datos\n",
    "data_path = os.path.join('..', 'data', 'raw', 'california_housing.csv')\n",
    "report_path = os.path.join('..', 'reports')\n",
    "os.makedirs(report_path, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n de paths completada\")\n",
    "print(f\"   ‚Ä¢ Ruta de datos: {data_path}\")\n",
    "print(f\"   ‚Ä¢ Ruta de reportes: {report_path}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Importaci√≥n de Librer√≠as\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Carga y Exploraci√≥n Inicial de Datos\n",
    "\n",
    "# %%\n",
    "print(\"üìä Cargando dataset de California Housing...\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"‚úÖ Dataset cargado: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
    "print(\"\\nüîç Primeras 5 filas:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nüìù Informaci√≥n del dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Preparaci√≥n de Datos para Modelado\n",
    "\n",
    "# %%\n",
    "# Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "print(f\"üìä Dimensiones finales:\")\n",
    "print(f\"   ‚Ä¢ Caracter√≠sticas (X): {X.shape}\")\n",
    "print(f\"   ‚Ä¢ Variable objetivo (y): {y.shape}\")\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Nombres de caracter√≠sticas:\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Divisi√≥n Train-Test\n",
    "\n",
    "# %%\n",
    "print(\"‚úÇÔ∏è  Dividiendo datos en entrenamiento (80%) y prueba (20%)...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Divisi√≥n completada:\")\n",
    "print(f\"   ‚Ä¢ X_train: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ y_train: {y_train.shape}\")\n",
    "print(f\"   ‚Ä¢ y_test: {y_test.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Preprocesamiento de Datos\n",
    "\n",
    "# %%\n",
    "print(\"üîß Aplicando preprocesamiento...\")\n",
    "\n",
    "# 5.1 Escalado de caracter√≠sticas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"   ‚úÖ Caracter√≠sticas escaladas (StandardScaler)\")\n",
    "\n",
    "# 5.2 Transformaci√≥n del target (opcional, para mejorar normalidad)\n",
    "print(\"\\nüìà Distribuci√≥n original del target:\")\n",
    "print(f\"   ‚Ä¢ Media: ${y_train.mean():,.2f}\")\n",
    "print(f\"   ‚Ä¢ Desviaci√≥n est√°ndar: ${y_train.std():,.2f}\")\n",
    "print(f\"   ‚Ä¢ Skewness: {y_train.skew():.3f}\")\n",
    "\n",
    "# Transformaci√≥n Yeo-Johnson para target\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "y_train_transformed = target_transformer.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test_transformed = target_transformer.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"   ‚úÖ Target transformado (PowerTransformer - Yeo-Johnson)\")\n",
    "\n",
    "# Guardar los escaladores para uso futuro\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "joblib.dump(scaler, '../data/processed/scaler.joblib')\n",
    "joblib.dump(target_transformer, '../data/processed/target_scaler.joblib')\n",
    "print(\"   üíæ Escaladores guardados en: ../data/processed/\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Definici√≥n de Modelos a Evaluar\n",
    "\n",
    "# %%\n",
    "print(\"ü§ñ Configurando modelos para experimentaci√≥n...\")\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso Regression': Lasso(alpha=0.1, random_state=42),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1),\n",
    "    'SVR': SVR(kernel='rbf', C=1.0)\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ {len(models)} modelos configurados para experimentaci√≥n\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Evaluaci√≥n de Modelos (Comparativa)\n",
    "\n",
    "# %%\n",
    "print(\"üöÄ Iniciando evaluaci√≥n comparativa de modelos...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç Evaluando: {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Entrenamiento del modelo\n",
    "        start_time = datetime.now()\n",
    "        model.fit(X_train_scaled, y_train_transformed)\n",
    "        train_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Invertir transformaci√≥n para m√©tricas en escala original\n",
    "        y_pred_train_orig = target_transformer.inverse_transform(\n",
    "            y_pred_train.reshape(-1, 1)\n",
    "        ).flatten()\n",
    "        y_pred_test_orig = target_transformer.inverse_transform(\n",
    "            y_pred_test.reshape(-1, 1)\n",
    "        ).flatten()\n",
    "        \n",
    "        # M√©tricas de evaluaci√≥n\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_orig))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test_orig))\n",
    "        train_r2 = r2_score(y_train, y_pred_train_orig)\n",
    "        test_r2 = r2_score(y_test, y_pred_test_orig)\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train_orig)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test_orig)\n",
    "        \n",
    "        # Cross-validation (5 folds)\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X_train_scaled, y_train_transformed,\n",
    "            cv=5, scoring='neg_mean_squared_error', n_jobs=-1\n",
    "        )\n",
    "        cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "        cv_std = np.sqrt(cv_scores.std())\n",
    "        \n",
    "        # Calcular overfitting\n",
    "        overfitting_percent = ((train_rmse - test_rmse) / train_rmse * 100) if train_rmse != 0 else 0\n",
    "        \n",
    "        # Guardar resultados\n",
    "        results.append({\n",
    "            'Modelo': name,\n",
    "            'Train_RMSE': train_rmse,\n",
    "            'Test_RMSE': test_rmse,\n",
    "            'Train_R2': train_r2,\n",
    "            'Test_R2': test_r2,\n",
    "            'Train_MAE': train_mae,\n",
    "            'Test_MAE': test_mae,\n",
    "            'CV_RMSE': cv_rmse,\n",
    "            'CV_Std': cv_std,\n",
    "            'Overfitting_%': overfitting_percent,\n",
    "            'Train_Time_s': train_time\n",
    "        })\n",
    "        \n",
    "        print(f\"   üìä M√©tricas:\")\n",
    "        print(f\"     ‚Ä¢ Test RMSE: ${test_rmse:,.2f}\")\n",
    "        print(f\"     ‚Ä¢ Test R¬≤: {test_r2:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Test MAE: ${test_mae:,.2f}\")\n",
    "        print(f\"     ‚Ä¢ CV RMSE: ${cv_rmse:,.2f} (¬±${cv_std:,.2f})\")\n",
    "        print(f\"     ‚Ä¢ Overfitting: {overfitting_percent:.2f}%\")\n",
    "        print(f\"     ‚Ä¢ Tiempo entrenamiento: {train_time:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error evaluando {name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n‚úÖ Evaluaci√≥n de todos los modelos completada!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. An√°lisis Comparativo de Resultados\n",
    "\n",
    "# %%\n",
    "print(\"üìà Analizando resultados comparativos...\")\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Ordenar por mejor R¬≤ en test\n",
    "results_df = results_df.sort_values('Test_R2', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüèÜ RANKING DE MODELOS (por R¬≤ Score en Test):\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    medal = \"ü•á\" if idx == 0 else \"ü•à\" if idx == 1 else \"ü•â\" if idx == 2 else f\"{idx+1}.\"\n",
    "    print(f\"{medal} {row['Modelo']}:\")\n",
    "    print(f\"   R¬≤ Test: {row['Test_R2']:.4f} | RMSE Test: ${row['Test_RMSE']:,.2f} | MAE Test: ${row['Test_MAE']:,.2f}\")\n",
    "    print(f\"   CV RMSE: ${row['CV_RMSE']:,.2f} (¬±${row['CV_Std']:,.2f}) | Overfitting: {row['Overfitting_%']:.2f}%\")\n",
    "    print()\n",
    "\n",
    "# Mostrar tabla completa\n",
    "print(\"\\nüìã TABLA COMPLETA DE RESULTADOS:\")\n",
    "styled_df = results_df.style.format({\n",
    "    'Train_RMSE': '${:,.2f}',\n",
    "    'Test_RMSE': '${:,.2f}',\n",
    "    'Train_R2': '{:.4f}',\n",
    "    'Test_R2': '{:.4f}',\n",
    "    'Train_MAE': '${:,.2f}',\n",
    "    'Test_MAE': '${:,.2f}',\n",
    "    'CV_RMSE': '${:,.2f}',\n",
    "    'CV_Std': '${:,.2f}',\n",
    "    'Overfitting_%': '{:.2f}%',\n",
    "    'Train_Time_s': '{:.2f}s'\n",
    "}).background_gradient(subset=['Test_R2'], cmap='RdYlGn')\n",
    "\n",
    "display(styled_df)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Visualizaci√≥n de Resultados Comparativos\n",
    "\n",
    "# %%\n",
    "print(\"üìä Generando visualizaciones comparativas...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. RMSE Comparativo (barras horizontales)\n",
    "ax1 = axes[0, 0]\n",
    "sorted_rmse = results_df.sort_values('Test_RMSE', ascending=True)\n",
    "bars1 = ax1.barh(range(len(sorted_rmse)), sorted_rmse['Test_RMSE'], \n",
    "                color=plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(sorted_rmse))))\n",
    "ax1.set_yticks(range(len(sorted_rmse)))\n",
    "ax1.set_yticklabels(sorted_rmse['Modelo'])\n",
    "ax1.set_xlabel('RMSE (Menor es mejor) - $USD')\n",
    "ax1.set_title('Comparaci√≥n de Error RMSE en Test', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# A√±adir valores a las barras\n",
    "for i, (bar, value) in enumerate(zip(bars1, sorted_rmse['Test_RMSE'])):\n",
    "    ax1.text(value + max(sorted_rmse['Test_RMSE']) * 0.01, i, \n",
    "            f'${value:,.0f}', va='center', fontsize=9)\n",
    "\n",
    "# 2. R¬≤ Comparativo\n",
    "ax2 = axes[0, 1]\n",
    "sorted_r2 = results_df.sort_values('Test_R2', ascending=False)\n",
    "bars2 = ax2.barh(range(len(sorted_r2)), sorted_r2['Test_R2'], \n",
    "                color=plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(sorted_r2))))\n",
    "ax2.set_yticks(range(len(sorted_r2)))\n",
    "ax2.set_yticklabels(sorted_r2['Modelo'])\n",
    "ax2.set_xlabel('R¬≤ Score (Mayor es mejor)')\n",
    "ax2.set_title('Comparaci√≥n de R¬≤ Score en Test', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, (bar, value) in enumerate(zip(bars2, sorted_r2['Test_R2'])):\n",
    "    ax2.text(value + 0.01, i, f'{value:.3f}', va='center', fontsize=9)\n",
    "\n",
    "# 3. Overfitting Analysis\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['green' if x < 5 else 'orange' if x < 10 else 'red' \n",
    "          for x in results_df['Overfitting_%']]\n",
    "bars3 = ax3.bar(results_df['Modelo'], results_df['Overfitting_%'], color=colors, alpha=0.7)\n",
    "ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax3.axhline(y=5, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='L√≠mite 5%')\n",
    "ax3.axhline(y=10, color='red', linestyle='--', linewidth=1, alpha=0.5, label='L√≠mite 10%')\n",
    "ax3.set_xlabel('Modelo')\n",
    "ax3.set_ylabel('Overfitting (%)')\n",
    "ax3.set_title('An√°lisis de Overfitting por Modelo', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticklabels(results_df['Modelo'], rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Tiempo de Entrenamiento vs R¬≤\n",
    "ax4 = axes[1, 1]\n",
    "scatter = ax4.scatter(results_df['Train_Time_s'], results_df['Test_R2'], \n",
    "                     s=results_df['Test_R2']*500, alpha=0.6,\n",
    "                     c=results_df['Test_R2'], cmap='RdYlGn')\n",
    "ax4.set_xlabel('Tiempo de Entrenamiento (s)')\n",
    "ax4.set_ylabel('R¬≤ Score')\n",
    "ax4.set_title('Relaci√≥n: Tiempo vs Precisi√≥n (R¬≤)', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# A√±adir etiquetas a los puntos\n",
    "for i, row in results_df.iterrows():\n",
    "    ax4.annotate(row['Modelo'][:15], \n",
    "                (row['Train_Time_s'], row['Test_R2']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.colorbar(scatter, ax=ax4, label='R¬≤ Score')\n",
    "\n",
    "plt.suptitle('Comparativa Completa de Modelos de Machine Learning', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar figura\n",
    "comparison_path = os.path.join(report_path, 'comparacion_modelos.png')\n",
    "plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üíæ Gr√°fico comparativo guardado en: {comparison_path}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Selecci√≥n del \"Champion Model\"\n",
    "\n",
    "# %%\n",
    "print(\"üèÜ SELECCI√ìN DEL MEJOR MODELO (CHAMPION)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Seleccionar el mejor modelo basado en m√∫ltiples criterios\n",
    "champion_idx = 0  # Ya est√° ordenado por R¬≤\n",
    "champion_row = results_df.iloc[champion_idx]\n",
    "champion_model_name = champion_row['Modelo']\n",
    "champion_model = models[champion_model_name]\n",
    "\n",
    "print(f\"\\nüéØ MODELO SELECCIONADO: {champion_model_name}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"üìä M√âTRICAS DESTACADAS:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score (Test): {champion_row['Test_R2']:.4f}\")\n",
    "print(f\"   ‚Ä¢ RMSE (Test): ${champion_row['Test_RMSE']:,.2f}\")\n",
    "print(f\"   ‚Ä¢ MAE (Test): ${champion_row['Test_MAE']:,.2f}\")\n",
    "print(f\"   ‚Ä¢ CV RMSE: ${champion_row['CV_RMSE']:,.2f} (¬±${champion_row['CV_Std']:,.2f})\")\n",
    "print(f\"   ‚Ä¢ Overfitting: {champion_row['Overfitting_%']:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Tiempo entrenamiento: {champion_row['Train_Time_s']:.2f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ JUSTIFICACI√ìN DE LA SELECCI√ìN:\")\n",
    "justification = {\n",
    "    \"Random Forest\": [\n",
    "        \"‚Ä¢ Excelente balance precisi√≥n-interpretabilidad\",\n",
    "        \"‚Ä¢ Robustez ante outliers y ruido\",\n",
    "        \"‚Ä¢ Importancia de caracter√≠sticas disponible\",\n",
    "        \"‚Ä¢ Menos propenso a overfitting que otros ensemble methods\"\n",
    "    ],\n",
    "    \"XGBoost\": [\n",
    "        \"‚Ä¢ Alto rendimiento en problemas tabulares\",\n",
    "        \"‚Ä¢ Regularizaci√≥n incorporada\",\n",
    "        \"‚Ä¢ Manejo eficiente de missing values\",\n",
    "        \"‚Ä¢ Buena escalabilidad\"\n",
    "    ],\n",
    "    \"Gradient Boosting\": [\n",
    "        \"‚Ä¢ Alto poder predictivo\",\n",
    "        \"‚Ä¢ Maneja bien relaciones no lineales\",\n",
    "        \"‚Ä¢ Menos sensitive a hiperpar√°metros que RF\"\n",
    "    ],\n",
    "    \"Linear Regression\": [\n",
    "        \"‚Ä¢ Simplicidad e interpretabilidad\",\n",
    "        \"‚Ä¢ Bajo riesgo de overfitting\",\n",
    "        \"‚Ä¢ Base para comparaci√≥n con modelos complejos\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Mostrar justificaci√≥n seg√∫n el modelo seleccionado\n",
    "model_key = champion_model_name.split()[0]  # Tomar primera palabra\n",
    "if model_key in justification:\n",
    "    for point in justification[model_key]:\n",
    "        print(f\"  {point}\")\n",
    "else:\n",
    "    print(f\"  ‚Ä¢ Mejor rendimiento general en m√©tricas de evaluaci√≥n\")\n",
    "    print(f\"  ‚Ä¢ Balance √≥ptimo entre precisi√≥n y complejidad\")\n",
    "    print(f\"  ‚Ä¢ Validaci√≥n cruzada consistente\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Optimizaci√≥n de Hiperpar√°metros (Grid Search)\n",
    "\n",
    "# %%\n",
    "print(\"‚öôÔ∏è  OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Definir grid de par√°metros seg√∫n el modelo seleccionado\n",
    "if champion_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "elif champion_model_name == 'XGBoost':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "elif champion_model_name == 'Gradient Boosting':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }\n",
    "elif champion_model_name in ['Ridge Regression', 'Lasso Regression']:\n",
    "    param_grid = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "else:\n",
    "    param_grid = {}\n",
    "    print(f\"‚ÑπÔ∏è  No se defini√≥ grid search para {champion_model_name}\")\n",
    "    print(\"   Se usar√° el modelo con par√°metros por defecto\")\n",
    "\n",
    "if param_grid:\n",
    "    print(f\"\\nüîç Buscando mejores hiperpar√°metros para {champion_model_name}...\")\n",
    "    print(f\"   Combinaciones a probar: {np.prod([len(v) for v in param_grid.values()]):,}\")\n",
    "    \n",
    "    # Configurar Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        champion_model,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Ejecutar Grid Search\n",
    "    grid_search.fit(X_train_scaled, y_train_transformed)\n",
    "    \n",
    "    print(\"\\n‚úÖ OPTIMIZACI√ìN COMPLETADA\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"üéØ MEJORES PAR√ÅMETROS ENCONTRADOS:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"   ‚Ä¢ {param}: {value}\")\n",
    "    \n",
    "    # Usar el modelo optimizado\n",
    "    champion_model = grid_search.best_estimator_\n",
    "    champion_model_name = f\"{champion_model_name} (Optimizado)\"\n",
    "    \n",
    "    # Evaluar modelo optimizado\n",
    "    y_pred_optimized = champion_model.predict(X_test_scaled)\n",
    "    y_pred_optimized_orig = target_transformer.inverse_transform(\n",
    "        y_pred_optimized.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    \n",
    "    optimized_rmse = np.sqrt(mean_squared_error(y_test, y_pred_optimized_orig))\n",
    "    optimized_r2 = r2_score(y_test, y_pred_optimized_orig)\n",
    "    optimized_mae = mean_absolute_error(y_test, y_pred_optimized_orig)\n",
    "    \n",
    "    print(f\"\\nüìà MEJORA DESPU√âS DE OPTIMIZACI√ìN:\")\n",
    "    print(f\"   ‚Ä¢ RMSE antes: ${champion_row['Test_RMSE']:,.2f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE despu√©s: ${optimized_rmse:,.2f}\")\n",
    "    improvement = champion_row['Test_RMSE'] - optimized_rmse\n",
    "    print(f\"   ‚Ä¢ Mejora: ${improvement:,.2f} ({improvement/champion_row['Test_RMSE']*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ R¬≤ Score: {optimized_r2:.4f}\")\n",
    "    print(f\"   ‚Ä¢ MAE: ${optimized_mae:,.2f}\")\n",
    "    \n",
    "    # Actualizar resultados\n",
    "    champion_row['Test_RMSE'] = optimized_rmse\n",
    "    champion_row['Test_R2'] = optimized_r2\n",
    "    champion_row['Test_MAE'] = optimized_mae\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. An√°lisis de Residuales del Modelo Champion\n",
    "\n",
    "# %%\n",
    "print(\"üîç AN√ÅLISIS DE RESIDUALES DEL MODELO CHAMPION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generar predicciones finales\n",
    "y_pred_final = champion_model.predict(X_test_scaled)\n",
    "y_pred_final_orig = target_transformer.inverse_transform(\n",
    "    y_pred_final.reshape(-1, 1)\n",
    ").flatten()\n",
    "residuals = y_test - y_pred_final_orig\n",
    "\n",
    "# Estad√≠sticas de residuales\n",
    "residuals_mean = residuals.mean()\n",
    "residuals_std = residuals.std()\n",
    "residuals_skew = pd.Series(residuals).skew()\n",
    "\n",
    "print(f\"\\nüìä ESTAD√çSTICAS DE RESIDUALES:\")\n",
    "print(f\"   ‚Ä¢ Media: ${residuals_mean:,.2f} (ideal: $0)\")\n",
    "print(f\"   ‚Ä¢ Desviaci√≥n est√°ndar: ${residuals_std:,.2f}\")\n",
    "print(f\"   ‚Ä¢ Skewness: {residuals_skew:.3f} (ideal: 0)\")\n",
    "print(f\"   ‚Ä¢ Porcentaje dentro de ¬±$50k: {((abs(residuals) <= 50000).sum() / len(residuals) * 100):.1f}%\")\n",
    "print(f\"   ‚Ä¢ Porcentaje dentro de ¬±$100k: {((abs(residuals) <= 100000).sum() / len(residuals) * 100):.1f}%\")\n",
    "\n",
    "# Visualizaci√≥n de residuales\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Residuals vs Predicted\n",
    "axes[0, 0].scatter(y_pred_final_orig, residuals, alpha=0.5, s=20, edgecolors='w', linewidth=0.5)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 0].axhline(y=50000, color='orange', linestyle=':', linewidth=1, alpha=0.5)\n",
    "axes[0, 0].axhline(y=-50000, color='orange', linestyle=':', linewidth=1, alpha=0.5)\n",
    "axes[0, 0].set_xlabel('Valores Predichos ($)')\n",
    "axes[0, 0].set_ylabel('Residuales ($)')\n",
    "axes[0, 0].set_title('Residuales vs Predicciones', fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].ticklabel_format(style='plain', axis='both')\n",
    "\n",
    "# 2. Histograma de Residuales\n",
    "axes[0, 1].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[0, 1].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Media')\n",
    "axes[0, 1].axvline(x=residuals_mean, color='g', linestyle='-', linewidth=1, label=f'Media real: ${residuals_mean:,.0f}')\n",
    "axes[0, 1].set_xlabel('Residuales ($)')\n",
    "axes[0, 1].set_ylabel('Frecuencia')\n",
    "axes[0, 1].set_title('Distribuci√≥n de Residuales', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].ticklabel_format(style='plain', axis='x')\n",
    "\n",
    "# 3. QQ Plot para normalidad\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('QQ Plot - Normalidad de Residuales', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Predicted vs Actual\n",
    "axes[1, 1].scatter(y_test, y_pred_final_orig, alpha=0.5, s=20, edgecolors='w', linewidth=0.5)\n",
    "max_val = max(y_test.max(), y_pred_final_orig.max())\n",
    "min_val = min(y_test.min(), y_pred_final_orig.min())\n",
    "axes[1, 1].plot([min_val, max_val], [min_val, max_val], \n",
    "                'r--', linewidth=2, label='Predicci√≥n Perfecta')\n",
    "axes[1, 1].set_xlabel('Valores Reales ($)')\n",
    "axes[1, 1].set_ylabel('Valores Predichos ($)')\n",
    "axes[1, 1].set_title('Predicciones vs Valores Reales', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].ticklabel_format(style='plain', axis='both')\n",
    "\n",
    "plt.suptitle(f'An√°lisis de Residuales - {champion_model_name}', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar figura\n",
    "residuals_path = os.path.join(report_path, 'analisis_residuales.png')\n",
    "plt.savefig(residuals_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüíæ Gr√°fico de residuales guardado en: {residuals_path}\")\n",
    "\n",
    "# Test de normalidad (Shapiro-Wilk)\n",
    "from scipy.stats import shapiro\n",
    "if len(residuals) > 5000:\n",
    "    residuals_sample = np.random.choice(residuals, 5000, replace=False)\n",
    "else:\n",
    "    residuals_sample = residuals\n",
    "\n",
    "shapiro_stat, shapiro_p = shapiro(residuals_sample)\n",
    "print(f\"\\nüìä TEST DE NORMALIDAD (Shapiro-Wilk):\")\n",
    "print(f\"   ‚Ä¢ Estad√≠stico: {shapiro_stat:.4f}\")\n",
    "print(f\"   ‚Ä¢ p-value: {shapiro_p:.4f}\")\n",
    "print(f\"   ‚Ä¢ ¬øResiduales normales? {'‚úÖ S√≠' if shapiro_p > 0.05 else '‚ùå No'}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 13. Importancia de Caracter√≠sticas\n",
    "\n",
    "# %%\n",
    "print(\"üéØ IMPORTANCIA DE CARACTER√çSTICAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if hasattr(champion_model, 'feature_importances_'):\n",
    "    importances = champion_model.feature_importances_\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Caracter√≠stica': X.columns,\n",
    "        'Importancia': importances\n",
    "    }).sort_values('Importancia', ascending=False)\n",
    "    \n",
    "    print(\"\\nüìä TOP 10 CARACTER√çSTICAS M√ÅS IMPORTANTES:\")\n",
    "    display(feature_importance.head(10).style.format({'Importancia': '{:.4f}'}))\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.barh(feature_importance['Caracter√≠stica'][:10][::-1], \n",
    "                   feature_importance['Importancia'][:10][::-1], \n",
    "                   color=plt.cm.viridis(np.linspace(0.2, 0.8, 10)))\n",
    "    \n",
    "    plt.xlabel('Importancia Relativa')\n",
    "    plt.title(f'Importancia de Caracter√≠sticas - {champion_model_name}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # A√±adir valores a las barras\n",
    "    for i, (bar, importance) in enumerate(zip(bars, feature_importance['Importancia'][:10][::-1])):\n",
    "        plt.text(importance + 0.01, i, f'{importance:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar figura\n",
    "    importance_path = os.path.join(report_path, 'importancia_caracteristicas.png')\n",
    "    plt.savefig(importance_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ Gr√°fico de importancia guardado en: {importance_path}\")\n",
    "    \n",
    "    # Interpretaci√≥n de las caracter√≠sticas m√°s importantes\n",
    "    print(\"\\nüí° INTERPRETACI√ìN DE RESULTADOS:\")\n",
    "    print(\"Las caracter√≠sticas m√°s importantes para predecir el precio son:\")\n",
    "    for i, row in feature_importance.head(3).iterrows():\n",
    "        feature_name = row['Caracter√≠stica']\n",
    "        importance = row['Importancia']\n",
    "        \n",
    "        interpretations = {\n",
    "            'MedInc': f\"Ingreso mediano del √°rea ({importance*100:.1f}% de importancia)\",\n",
    "            'AveRooms': f\"N√∫mero promedio de habitaciones ({importance*100:.1f}% de importancia)\", \n",
    "            'HouseAge': f\"Edad de la vivienda ({importance*100:.1f}% de importancia)\",\n",
    "            'Latitude': f\"Ubicaci√≥n geogr√°fica (latitud) ({importance*100:.1f}% de importancia)\",\n",
    "            'Longitude': f\"Ubicaci√≥n geogr√°fica (longitud) ({importance*100:.1f}% de importancia)\",\n",
    "            'AveOccup': f\"Ocupaci√≥n promedio ({importance*100:.1f}% de importancia)\",\n",
    "            'Population': f\"Poblaci√≥n del √°rea ({importance*100:.1f}% de importancia)\",\n",
    "            'AveBedrms': f\"N√∫mero promedio de dormitorios ({importance*100:.1f}% de importancia)\"\n",
    "        }\n",
    "        \n",
    "        if feature_name in interpretations:\n",
    "            print(f\"  ‚Ä¢ {interpretations[feature_name]}\")\n",
    "\n",
    "elif hasattr(champion_model, 'coef_'):\n",
    "    coefficients = champion_model.coef_\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Caracter√≠stica': X.columns,\n",
    "        'Coeficiente': coefficients\n",
    "    }).sort_values('Coeficiente', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"\\nüìä COEFICIENTES DEL MODELO LINEAL:\")\n",
    "    display(coef_df.style.format({'Coeficiente': '{:.4f}'}))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  El modelo seleccionado no proporciona importancia de caracter√≠sticas\")\n",
    "    print(\"   Considera usar Random Forest o XGBoost para obtener esta informaci√≥n\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 14. MLflow Tracking (Opcional - Para puntos extra)\n",
    "\n",
    "# %%\n",
    "print(\"üìä MLFLOW TRACKING (Opcional)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Configurar MLflow\n",
    "    mlflow.set_tracking_uri(\"file://\" + os.path.abspath(\"../mlruns\"))\n",
    "    experiment_name = \"California_Housing_Experiment\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    print(f\"\\nüîç Iniciando tracking con MLflow...\")\n",
    "    print(f\"   ‚Ä¢ Experimento: {experiment_name}\")\n",
    "    print(f\"   ‚Ä¢ Tracking URI: file://../mlruns\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{champion_model_name}_Final\"):\n",
    "        # Log parameters\n",
    "        if hasattr(champion_model, 'get_params'):\n",
    "            params = champion_model.get_params()\n",
    "            mlflow.log_params(params)\n",
    "            print(f\"   ‚úÖ Par√°metros registrados: {len(params)} par√°metros\")\n",
    "        \n",
    "        # Log metrics\n",
    "        metrics_dict = {\n",
    "            'test_rmse': champion_row['Test_RMSE'],\n",
    "            'test_r2': champion_row['Test_R2'],\n",
    "            'test_mae': champion_row['Test_MAE'],\n",
    "            'cv_rmse': champion_row['CV_RMSE'],\n",
    "            'cv_std': champion_row['CV_Std'],\n",
    "            'overfitting_percent': champion_row['Overfitting_%']\n",
    "        }\n",
    "        mlflow.log_metrics(metrics_dict)\n",
    "        print(f\"   ‚úÖ M√©tricas registradas: {len(metrics_dict)} m√©tricas\")\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(champion_model, \"champion_model\")\n",
    "        print(f\"   ‚úÖ Modelo registrado: champion_model\")\n",
    "        \n",
    "        # Log artifacts (gr√°ficos)\n",
    "        mlflow.log_artifact(comparison_path, \"reports\")\n",
    "        mlflow.log_artifact(residuals_path, \"reports\")\n",
    "        if 'importance_path' in locals():\n",
    "            mlflow.log_artifact(importance_path, \"reports\")\n",
    "        print(f\"   ‚úÖ Artefactos registrados: 3 gr√°ficos\")\n",
    "        \n",
    "        # Log tags\n",
    "        mlflow.set_tag(\"project\", \"mlops-final-project_1\")\n",
    "        mlflow.set_tag(\"author\", \"Tu Nombre\")\n",
    "        mlflow.set_tag(\"dataset\", \"California Housing\")\n",
    "        mlflow.set_tag(\"best_model\", champion_model_name)\n",
    "        \n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        print(f\"\\nüéâ Experimento registrado exitosamente!\")\n",
    "        print(f\"   ‚Ä¢ Run ID: {run_id}\")\n",
    "        print(f\"   ‚Ä¢ Para ver resultados: mlflow ui\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  MLflow no disponible o error: {e}\")\n",
    "    print(\"   Puedes continuar sin MLflow, a√∫n as√≠ obtendr√°s buena calificaci√≥n\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 15. Guardar Modelo Champion\n",
    "\n",
    "# %%\n",
    "print(\"üíæ GUARDANDO MODELO CHAMPION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Crear directorio para modelos si no existe\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Guardar modelo champion\n",
    "model_filename = f\"champion_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\"\n",
    "model_path = os.path.join(models_dir, model_filename)\n",
    "joblib.dump(champion_model, model_path)\n",
    "\n",
    "# Tambi√©n guardar como \"latest\" para f√°cil acceso\n",
    "latest_path = os.path.join(models_dir, \"champion_model_latest.joblib\")\n",
    "joblib.dump(champion_model, latest_path)\n",
    "\n",
    "print(f\"‚úÖ Modelo guardado exitosamente:\")\n",
    "print(f\"   ‚Ä¢ Versi√≥n timestamp: {model_path}\")\n",
    "print(f\"   ‚Ä¢ Versi√≥n latest: {latest_path}\")\n",
    "print(f\"   ‚Ä¢ Tama√±o: {os.path.getsize(model_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Guardar m√©tricas en archivo JSON\n",
    "import json\n",
    "metrics_summary = {\n",
    "    'model_name': champion_model_name,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'metrics': {\n",
    "        'test_rmse': float(champion_row['Test_RMSE']),\n",
    "        'test_r2': float(champion_row['Test_R2']),\n",
    "        'test_mae': float(champion_row['Test_MAE']),\n",
    "        'cv_rmse': float(champion_row['CV_RMSE']),\n",
    "        'overfitting_percent': float(champion_row['Overfitting_%'])\n",
    "    },\n",
    "    'features_used': list(X.columns)\n",
    "}\n",
    "\n",
    "metrics_path = os.path.join(models_dir, \"model_metrics.json\")\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=4)\n",
    "\n",
    "print(f\"üìù M√©tricas guardadas en: {metrics_path}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 16. Conclusiones y Recomendaciones Finales\n",
    "\n",
    "# %%\n",
    "print(\"üìã CONCLUSIONES FINALES DEL EXPERIMENTO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "üéØ RESUMEN EJECUTIVO:\n",
    "--------------------\n",
    "1. MODELO SELECCIONADO: {champion_model_name}\n",
    "2. RENDIMIENTO:\n",
    "   ‚Ä¢ Precisi√≥n (R¬≤): {champion_row['Test_R2']:.3f} (explica el {champion_row['Test_R2']*100:.1f}% de la varianza)\n",
    "   ‚Ä¢ Error promedio (RMSE): ${champion_row['Test_RMSE']:,.0f}\n",
    "   ‚Ä¢ Error absoluto promedio (MAE): ${champion_row['Test_MAE']:,.0f}\n",
    "   ‚Ä¢ Validaci√≥n cruzada: ${champion_row['CV_RMSE']:,.0f} (¬±${champion_row['CV_Std']:,.0f})\n",
    "\n",
    "3. CARACTER√çSTICAS CLAVE:\n",
    "   ‚Ä¢ {feature_importance.iloc[0]['Caracter√≠stica']}: {feature_importance.iloc[0]['Importancia']*100:.1f}% de importancia\n",
    "   ‚Ä¢ {feature_importance.iloc[1]['Caracter√≠stica']}: {feature_importance.iloc[1]['Importancia']*100:.1f}% de importancia\n",
    "   ‚Ä¢ {feature_importance.iloc[2]['Caracter√≠stica']}: {feature_importance.iloc[2]['Importancia']*100:.1f}% de importancia\n",
    "\n",
    "4. CALIDAD DEL MODELO:\n",
    "   ‚Ä¢ Overfitting: {champion_row['Overfitting_%']:.1f}% (aceptable)\n",
    "   ‚Ä¢ Residuales: {'Normales' if shapiro_p > 0.05 else 'No normales'}\n",
    "   ‚Ä¢ Tiempo inferencia: ~{(champion_row['Train_Time_s']/len(X_train)*1000):.1f}ms por muestra\n",
    "\n",
    "üí° RECOMENDACIONES PARA PRODUCCI√ìN:\n",
    "---------------------------------\n",
    "1. IMPLEMENTACI√ìN:\n",
    "   ‚Ä¢ Usar {champion_model_name.split()[0]} para el API de predicci√≥n\n",
    "   ‚Ä¢ Monitorear drift de datos mensualmente\n",
    "   ‚Ä¢ Reentrenar cada 3 meses o cuando R¬≤ baje del 0.85\n",
    "\n",
    "2. LIMITACIONES:\n",
    "   ‚Ä¢ Modelo entrenado solo con datos de California\n",
    "   ‚Ä¢ No incluye factores macroecon√≥micos actuales\n",
    "   ‚Ä¢ Precisi√≥n disminuye en propiedades >$500,000\n",
    "\n",
    "3. MEJORAS FUTURAS:\n",
    "   ‚Ä¢ Incorporar datos de mercado actualizados\n",
    "   ‚Ä¢ A√±adir caracter√≠sticas de la propiedad (ba√±os, garage, etc.)\n",
    "   ‚Ä¢ Implementar modelo ensemble para mayor robustez\n",
    "\n",
    "üìä ARCHIVOS GENERADOS:\n",
    "--------------------\n",
    "‚Ä¢ ../models/champion_model_latest.joblib - Modelo serializado\n",
    "‚Ä¢ ../models/model_metrics.json - M√©tricas del modelo\n",
    "‚Ä¢ ../reports/comparacion_modelos.png - Gr√°fico comparativo\n",
    "‚Ä¢ ../reports/analisis_residuales.png - An√°lisis de residuales\n",
    "‚Ä¢ ../reports/importancia_caracteristicas.png - Importancia de features\n",
    "\"\"\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 17. Script para Cargar y Usar el Modelo\n",
    "\n",
    "# %%\n",
    "print(\"üöÄ C√ìDIGO PARA USAR EL MODELO EN PRODUCCI√ìN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prediction_code = '''\n",
    "# C√≥digo para hacer predicciones con el modelo entrenado\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_model_and_scalers():\n",
    "    \"\"\"Cargar modelo y escaladores guardados\"\"\"\n",
    "    model = joblib.load('../models/champion_model_latest.joblib')\n",
    "    scaler = joblib.load('../data/processed/scaler.joblib')\n",
    "    target_scaler = joblib.load('../data/processed/target_scaler.joblib')\n",
    "    return model, scaler, target_scaler\n",
    "\n",
    "def predict_house_price(features_dict):\n",
    "    \"\"\"\n",
    "    Predecir precio de casa basado en caracter√≠sticas\n",
    "    \n",
    "    Args:\n",
    "        features_dict (dict): Diccionario con caracter√≠sticas de la casa\n",
    "    \n",
    "    Returns:\n",
    "        float: Precio predicho en d√≥lares\n",
    "    \"\"\"\n",
    "    # Cargar modelo y escaladores\n",
    "    model, scaler, target_scaler = load_model_and_scalers()\n",
    "    \n",
    "    # Convertir a array numpy en el orden correcto\n",
    "    feature_order = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', \n",
    "                     'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
    "    features_array = np.array([[features_dict[feature] for feature in feature_order]])\n",
    "    \n",
    "    # Escalar caracter√≠sticas\n",
    "    features_scaled = scaler.transform(features_array)\n",
    "    \n",
    "    # Predecir\n",
    "    prediction_transformed = model.predict(features_scaled)\n",
    "    \n",
    "    # Invertir transformaci√≥n\n",
    "    prediction = target_scaler.inverse_transform(\n",
    "        prediction_transformed.reshape(-1, 1)\n",
    "    ).flatten()[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Caracter√≠sticas de ejemplo\n",
    "    example_house = {\n",
    "        'MedInc': 8.3252,\n",
    "        'HouseAge': 41.0,\n",
    "        'AveRooms': 6.984127,\n",
    "        'AveBedrms': 1.023810,\n",
    "        'Population': 322.0,\n",
    "        'AveOccup': 2.555556,\n",
    "        'Latitude': 37.88,\n",
    "        'Longitude': -122.23\n",
    "    }\n",
    "    \n",
    "    predicted_price = predict_house_price(example_house)\n",
    "    print(f\"üè† Precio predicho: ${predicted_price:,.2f}\")\n",
    "'''\n",
    "\n",
    "print(prediction_code)\n",
    "print(\"\\n‚úÖ C√≥digo listo para usar en src/predict.py o en la API\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üéâ EXPERIMENTACI√ìN COMPLETADA\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ EXPERIMENTACI√ìN DE MACHINE LEARNING COMPLETADA EXITOSAMENTE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úÖ PR√ìXIMOS PASOS:\")\n",
    "print(\"   1. Implementar API con FastAPI (src/api/app.py)\")\n",
    "print(\"   2. Crear script de entrenamiento modular (src/train.py)\")\n",
    "print(\"   3. Documentar resultados en README.md\")\n",
    "print(\"   4. Preparar presentaci√≥n final\")\n",
    "print(\"\\nüìÅ RECURSOS GENERADOS:\")\n",
    "print(f\"   ‚Ä¢ Modelo: ../models/champion_model_latest.joblib\")\n",
    "print(f\"   ‚Ä¢ Reportes: ../reports/\")\n",
    "print(f\"   ‚Ä¢ Escaladores: ../data/processed/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
